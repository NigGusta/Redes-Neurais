ETAPAS PARA REALIZAÇÃO:


Carregamento e Pré-Processamento dos Dados:

Importando as bibliotecas necessárias, como scikit-learn, pandas e nltk.
Carregando o conjunto de dados.
Pré-processando os dados, realizando o tratamento e limpeza dos dados.


Tokenização e Vetorização:

Tokenizar o texto para dividir as palavras em tokens individuais após serem tratados.
utilizando o TF-IDF (Term Frequency-Inverse Document Frequency) para converter os tokens em representações numéricas.


Treinamento do Modelo:

Usando o algoritmo de classificação, Naive Bayes para treinar o modelo com os dados de treinamento.


Avaliação do Modelo:

Usando os dados de teste para avaliar o desempenho do modelo treinado.
Calculando as métricas de avaliação, como acurácia, precisão, recall e pontuação F1.

